{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "client = OpenAI()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "  api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_extracted_topics(model, extraction_template1, extraction_template2):\n",
    "  \"\"\"Return the extracted topics from the text\"\"\"\n",
    "  try:\n",
    "    extraction_completion1 = client.chat.completions.create(\n",
    "    model=model, # \"gpt-3.5-turbo-0125\" or \"gpt-4-0125-preview\"\n",
    "    temperature=0.0, # 0.0 is deterministic, 1.0 is maximum randomness\n",
    "    top_p=0.0, # 0.0 is deterministic, 1.0 is maximum randomness\n",
    "    seed=100, # Seed is used to make sure that the same prompt will always generate the same completion\n",
    "    messages=[\n",
    "        # {\"role\": \"user\", \"content\": extraction_template1},\n",
    "        {\"role\": \"user\", \"content\": extraction_template1}\n",
    "      ]\n",
    "    )\n",
    "  except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    return None\n",
    "  \n",
    "  try:\n",
    "    extraction_completion2 = client.chat.completions.create(\n",
    "    model=model, # \"gpt-3.5-turbo-0125\" or \"gpt-4-0125-preview\"\n",
    "    temperature=0.0, # 0.0 is deterministic, 1.0 is maximum randomness\n",
    "    top_p=0.0, # 0.0 is deterministic, 1.0 is maximum randomness\n",
    "    seed=100, # Seed is used to make sure that the same prompt will always generate the same completion\n",
    "    messages=[\n",
    "        # {\"role\": \"user\", \"content\": extraction_template1},\n",
    "        {\"role\": \"user\", \"content\": extraction_template2}\n",
    "      ]\n",
    "    )\n",
    "  except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    return None\n",
    "  \n",
    "  return extraction_completion1.choices[0].message.content, extraction_completion2.choices[0].message.content\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summarized_text(model, summarization_template):\n",
    "    \"\"\"Return the summarized text from the extracted text\"\"\"\n",
    "    summarization_completion = client.chat.completions.create(\n",
    "    model=model, # \"gpt-3.5-turbo-0125\" or \"gpt-4-0125-preview\"\n",
    "    temperature=0.0,\n",
    "    top_p=0.0,\n",
    "    seed=200, # Seed is used to make sure that the same prompt will always generate the same completion\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": summarization_template}\n",
    "        ]\n",
    "    )\n",
    "    return summarization_completion.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_template1 = \"\"\"You are a smart principal investigator for a clinical trial, and I will give you the text of the consent form. Below are questions separated by |. I want you to get the full exact text for each question if it's in the form else return \"na\".\n",
    "Provide responses with the exact text from the form in defined format. Rephrasing text is not allowed, and you should not skip over information on the form.\n",
    "Points: [1. Does the study involve medical research?|\n",
    "2. What is the purpose of this research study?|\n",
    "3. How long will the participant be involved in the study?|\n",
    "4. What procedures will the participant need to follow?|\n",
    "5. Are any of the procedures experimental?|\n",
    "6. What are the risks or discomforts of participating?|\n",
    "7. What are the benefits of participating?|\n",
    "8. How many people will be participating in the study?]\n",
    "\n",
    "response1 = {{\"study_research\": exact_text_1,\n",
    "\"purpose\": exact_text_2,\n",
    "\"duration\": exact_text_3,\n",
    "\"procedures\": exact_text_4,\n",
    "\"experimental_procedures\": exact_text_5,\n",
    "\"risks\": exact_text_6,\n",
    "\"benefits\": exact_text_7,\n",
    "\"participants\": exact_text_8}}\n",
    " \n",
    "Be consistent with the naming of the keys and the format of the values.\n",
    "\n",
    "Form text:  {form_text}\"\"\"\n",
    "\n",
    "\n",
    "extraction_template2 = \"\"\"You are a smart principal investigator for a clinical trial, and use same text of the consent form from previous message. Below are questions separated by |. I want you to get the full exact text for each question if it's in the form else return \"na\".\n",
    "Provide responses with the exact text from the form in the defined format. Rephrasing text is not allowed, and you should not skip over information on the form.\n",
    "Points: [9. What other treatment options may help the participant besides this research study?|\n",
    "10. How will the researchers keep the participant's records private?|\n",
    "11. What are details on compensation, medical treatments, injuries, and where individuals can find more information?|\n",
    "12. Who can the participant contact if they have questions or get hurt in the research?|\n",
    "13. Can the participant drop out of the study anytime without consequences?|\n",
    "14. For what reasons could the researchers remove the participant from the study?|\n",
    "15. Will participating cost the participant anything?|\n",
    "16. If the participant wants to stop the study, what should they do and what happens next?|\n",
    "17. Will the researchers let the participant know if they find anything important that could change their decision to stay in the study?]\n",
    "\n",
    "response2 = {{\"alternative_procedures\": exact_text_1,\n",
    "\"confidentiality\": exact_text_2,\n",
    "\"compensation\": exact_text_3,\n",
    "\"contact_info\": exact_text_4,\n",
    "\"voluntary_participation\": exact_text_5,\n",
    "\"discontinue_cooperation\": exact_text_6,\n",
    "\"additional_costs\": exact_text_7,\n",
    "\"withdrawal_effects\": exact_text_8,\n",
    "\"new_findings\": exact_text_9}}\n",
    "\n",
    "Be consistent with the naming of the keys and the format of the values.\n",
    "\n",
    "Form text:  {form_text}\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of 11 trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Move up two levels to get the folder path\n",
    "two_levels_up = os.path.dirname(os.path.dirname(current_directory))\n",
    "\n",
    "# # Combine the two-level-up directory with the folder name to get the complete path\n",
    "folder_path = os.path.join(two_levels_up, r\"Data\\ICF\\11trials_for_analysis\")\n",
    "files = os.listdir(folder_path)\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "def count_tokens(text):\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "    token_count = len(encoding.encode(text))\n",
    "    return token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    if file.endswith(\".txt\"):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        with open(os.path.join(folder_path, file), \"r\", encoding=\"utf8\") as f:\n",
    "        # with open(os.path.join(folder_path, file), \"r\", encoding=\"ISO-8859-1\") as f:\n",
    "            form_text = f.read()\n",
    "            print(f\"File {file } has {count_tokens(form_text)} tokens\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"gpt-4-0125-preview\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=[\"file_name\", \"token_count\", \"extracted_topics1\", \"extracted_topics2\",\n",
    "                        \"study_research\", \"purpose\", \"duration\", \"procedures\", \"experimental_procedures\", \"risks\", \"benefits\", \"participants\",\n",
    "                        \"alternative_procedures\", \"confidentiality\", \"compensation\", \"contact_info\", \"voluntary_participation\", \"discontinue_cooperation\", \"additional_costs\", \"withdrawal_effects\", \"new_findings\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for file in files:\n",
    "    if file.endswith(\".txt\"):\n",
    "        with open(os.path.join(folder_path, file), \"r\", encoding=\"utf8\") as f:\n",
    "            form_text = f.read()\n",
    "            token_count = count_tokens(form_text)          \n",
    "            extracted_content1, extracted_content2 = get_extracted_topics(model, extraction_template1.format(form_text=form_text), extraction_template2.format(form_text=form_text))\n",
    "            print(\"\\n\\n\")\n",
    "            df = df.append({\"file_name\": file.split('.')[0], \"token_count\": token_count, \"extracted_topics1\": extracted_content1, \"extracted_topics2\":extracted_content2}, ignore_index=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate the extracted topics into different columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "for row in df.iterrows():\n",
    "    # print(row[1]['extracted_topics1'])  \n",
    "    if (\"= {\" in row[1]['extracted_topics1']):\n",
    "        response1 = row[1]['extracted_topics1'].split(\"=\")[1]\n",
    "        response1 = response1[:response1.index('\"}')+2]\n",
    "        json_response1 = json.loads(response1)\n",
    "    else:\n",
    "        response1 = row[1]['extracted_topics1']\n",
    "        response1 = response1[response1.index(\"{\\n\"):response1.index(\"}\")+1]\n",
    "        json_response1 = json.loads(response1)\n",
    "    # print(json_response1)\n",
    "\n",
    "    if (\"= {\" in row[1]['extracted_topics2']):\n",
    "        response2 = row[1]['extracted_topics2'].split(\"=\")[1]\n",
    "        response2 = response2[:response2.index('\"}')+2]\n",
    "        json_response2 = json.loads(response2)\n",
    "    else:\n",
    "        response2 = row[1]['extracted_topics2']\n",
    "        response2 = response2[response2.index(\"{\\n\"):response2.index(\"}\")+1]\n",
    "        json_response2 = json.loads(response2)\n",
    "    # print(json_response2)\n",
    "    \n",
    "    # Select the index of the row you want to update\n",
    "    row_index = row[0]\n",
    "\n",
    "    # Update the specified row with data from JSON\n",
    "    for key, value in json_response1.items():\n",
    "        if key in df.columns:\n",
    "            df.at[row_index, key] = value\n",
    "    \n",
    "    for key, value in json_response2.items():\n",
    "        if key in df.columns:\n",
    "            df.at[row_index, key] = value\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(folder_path+\"\\extracted_11trials_gpt4.xlsx\", index=False, engine='openpyxl')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>PoC for summaries with two ways</h4>\n",
    "Approach 1: Using extracted topics from ICF\n",
    "\n",
    "Approach 2: Using form text directly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mention file name according to the model used\n",
    "df = pd.read_excel(folder_path+\"\\extracted_11trials_gpt4.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt v5 from prompt excel file\n",
    "summary_template_approach1 = \"\"\"As the intelligent principal investigator of a clinical trial, you've received extracted text from the consent form. You must provide a clear summary.\n",
    "\n",
    "The summary should include a statement that explain the purpose of the research, a description of the procedures, how long the subject will be involved in the study, the risks, the benefits, \n",
    "the subject's participation is voluntary, and the alternatives if the subject decides not to participate. \n",
    "\n",
    "Use the following guidelines to create a summary in a paragraph style:\n",
    "- Summary must be at most 150 words.\n",
    "- Simplify any complex terms or concepts.\n",
    "- Make the summary highly understandable, recommended for eighth-grade level audience.\n",
    "- Use respectful and empowering language for patients.\n",
    "- Spell out acronyms upon first use.\n",
    "- Include all relevant information without adding extra details.\n",
    "- Use active voice\n",
    "- Keep the summary concise and to the point\n",
    "\n",
    "Your extracted text is: {extracted_content}\"\"\"\n",
    "\n",
    "summary_template_approach2 = \"\"\"As an intelligent principal investigator of a clinical trial, you must provide a clear summary using the consent form text.\n",
    "\n",
    "The summary should include a statement that explain the purpose of the research, a description of the procedures, how long the subject will be involved in the study, the risks, the benefits, \n",
    "the subject's participation is voluntary, and the alternatives if the subject decides not to participate. \n",
    "\n",
    "Use the following guidelines to create a summary in a paragraph style:\n",
    "- Summary must be at most 150 words.\n",
    "- Simplify any complex terms or concepts.\n",
    "- Make the summary highly understandable, recommended for eighth-grade level audience.\n",
    "- Use respectful and empowering language for patients.\n",
    "- Spell out acronyms upon first use.\n",
    "- Include all relevant information without adding extra details.\n",
    "- Use active voice.\n",
    "- Keep the summary concise and to the point.\n",
    "\n",
    "Form Text: {form_text}\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_approach1 = pd.DataFrame(columns=[\"file_name\", \"summarized_text_approach1\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for file in files:\n",
    "    if file.endswith(\".txt\"):\n",
    "        with open(os.path.join(folder_path, file), \"r\", encoding=\"utf8\") as f:\n",
    "            # form_text = f.read()\n",
    "            file_name = file.split('.')[0]\n",
    "            purpose = df[df['file_name'] == file_name]['purpose'].values[0]\n",
    "            procedures = df[df['file_name'] == file_name]['procedures'].values[0]\n",
    "            duration = df[df['file_name'] == file_name]['duration'].values[0]\n",
    "            risks = df[df['file_name'] == file_name]['risks'].values[0]\n",
    "            benefits = df[df['file_name'] == file_name]['benefits'].values[0]\n",
    "            alternative_procedures = df[df['file_name'] == file_name]['alternative_procedures'].values[0]\n",
    "            voluntary_participation = df[df['file_name'] == file_name]['voluntary_participation'].values[0]\n",
    "\n",
    "            extracted_content = f\"{purpose} {procedures} {duration} {risks} {benefits} {alternative_procedures} {voluntary_participation}\"\n",
    "\n",
    "            # print(extracted_content)\n",
    "            summary = get_summarized_text(model, summary_template_approach1.format(extracted_content=extracted_content))\n",
    "            print(summary)\n",
    "            print(\"\\n\")\n",
    "            df_approach1 = df_approach1.append({\"file_name\": file_name, \"summarized_text_approach1\": summary}, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_approach1['summarized_text_approach1'] = df_approach1['summarized_text_approach1'].apply(lambda x: x.replace(\"\\n\", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_approach1.to_excel('summarized_10trials_gpt4-approach1.xlsx', index=False, engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_approach2 = pd.DataFrame(columns=[\"file_name\", \"summarized_text_approach2\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for file in files:\n",
    "    if file.endswith(\".txt\"):\n",
    "        with open(os.path.join(folder_path, file), \"r\", encoding=\"utf8\") as f:\n",
    "            form_text = f.read()\n",
    "            file_name = file.split('.')[0]\n",
    "            \n",
    "            summary = get_summarized_text(model, summary_template_approach2.format(form_text=form_text))\n",
    "            print(summary)\n",
    "            print(\"\\n\")\n",
    "            df_approach2 = df_approach2.append({\"file_name\": file_name, \"summarized_text_approach2\": summary}, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_approach2['summarized_text_approach2'] = df_approach2['summarized_text_approach2'].apply(lambda x: x.replace(\"\\n\", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_approach = pd.merge(df_approach1, df_approach2, on=\"file_name\")\n",
    "df_new_approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_approach.to_excel('summary_trials_gpt4_031124.xlsx', index=False, engine='openpyxl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "harvard",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9c14e859c96f5b24484776f32de0563c002ed3057a960f3388a96d6055ade18f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
