{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BaF_zpo-qW5f"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import base64\n",
        "import os\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import glob\n",
        "from collections import defaultdict\n",
        "import time\n",
        "from azure.identity import DefaultAzureCredential, get_bearer_token_provider"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fetching info from clinicaltrial.gov using API"
      ],
      "metadata": {
        "id": "1qs1DNyLqtvg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p '/content/drive/MyDrive/ChatNCCN/cancer_icfs_21to24'\n",
        "pattern = r\"https://\\S*/ICF_0\\d\\d+\\.pdf\"\n",
        "\n",
        "for i in cancer_df_21to24.iterrows():\n",
        "  nct_id = i[1]['NCT Number']\n",
        "  urls = urls = re.findall(pattern, i[1]['Study Documents'])\n",
        "  if len(urls) > 1 and (nct_id == 'NCT04789720' or nct_id == 'NCT06183788' or nct_id == 'NCT04975698'):\n",
        "    urls = [urls[0]]\n",
        "  print(urls)\n",
        "  for j, url in enumerate(urls):\n",
        "    r = requests.get(url)\n",
        "    if len(urls) > 1:\n",
        "      with open(f'/content/drive/MyDrive/ChatNCCN/cancer_icfs_21to24/{nct_id}_{j}.pdf', 'wb') as f:\n",
        "        f.write(r.content)\n",
        "    else:\n",
        "      with open(f'/content/drive/MyDrive/ChatNCCN/cancer_icfs_21to24/{nct_id}.pdf', 'wb') as f:\n",
        "        f.write(r.content)"
      ],
      "metadata": {
        "id": "he-r7vw3qd-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import expert-created MCQAs examples"
      ],
      "metadata": {
        "id": "XGuuzHYzq4K2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_multiple_choices_questions = pd.read_excel('/content/drive/MyDrive/ChatNCCN/Example MCQA test for NCT03923790.xlsx')\n",
        "example_multiple_choices_questions"
      ],
      "metadata": {
        "id": "GzR7Dr0Cq0wr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Processing the given examples"
      ],
      "metadata": {
        "id": "v4dW3ijxrYzj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "questions = example_multiple_choices_questions['Question'][example_multiple_choices_questions['Question'] != 'N/A (does not appear in consent)']\n",
        "options = example_multiple_choices_questions['Multi-Choice Options'].dropna()\n",
        "topics = example_multiple_choices_questions['Topic']\n",
        "print(len(questions), len(options), len(topics))\n",
        "\n",
        "topics_notna = topics.dropna()\n",
        "topics_notna_index = set(topics_notna.index)\n",
        "i = 0\n",
        "while i < len(topics):\n",
        "  if i in topics_notna_index:\n",
        "    current_topic = topics.iloc[i].strip()\n",
        "    i += 1\n",
        "    while i < len(topics) and i not in topics_notna_index:\n",
        "      topics.iloc[i] = current_topic\n",
        "      i += 1\n",
        "topics = topics.iloc[questions.index]\n",
        "topics"
      ],
      "metadata": {
        "id": "R-PFQhpAq3xs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p '/content/drive/MyDrive/ChatNCCN/nct_mcqa_cancer_icfs'\n",
        "out_dir = '/content/drive/MyDrive/ChatNCCN/nct_mcqa_cancer_icfs'\n",
        "in_dir = '/content/drive/MyDrive/ChatNCCN'\n",
        "\n",
        "# importing required modules\n",
        "from PyPDF2 import PdfReader\n",
        "\n",
        "def convert_pdf_to_text(pdf_path):\n",
        "  # creating a pdf reader object\n",
        "  reader = PdfReader(pdf_path)\n",
        "\n",
        "  # printing number of pages in pdf file\n",
        "  print(len(reader.pages))\n",
        "\n",
        "  example_consent_form = ''\n",
        "  for i in range(len(reader.pages)):\n",
        "    page_text = reader.pages[i].extract_text()\n",
        "    if page_text == '':\n",
        "      return ''\n",
        "    example_consent_form += page_text.strip()\n",
        "  return example_consent_form"
      ],
      "metadata": {
        "id": "haZuoREprhAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get text of the example ICF (used for creating exmaple MCQAs)"
      ],
      "metadata": {
        "id": "DFbuouGcrvwN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_consent_form = convert_pdf_to_text(os.path.join(in_dir, 'example_consent_form_nct03923790.pdf'))\n",
        "example_consent_form"
      ],
      "metadata": {
        "id": "w-q17-BirvBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Select NIH topics that show up in the example ICF"
      ],
      "metadata": {
        "id": "guPxeUzDr6o7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topics_notna_index_set = set(list(topics_notna_index))\n",
        "questions_index_set = set(list(questions.index))\n",
        "intersect_set = topics_notna_index_set.intersection(questions_index_set)\n",
        "interse_set = intersect_set.intersection(set(questions))\n",
        "print(intersect_set)"
      ],
      "metadata": {
        "id": "1d2EYLmNr5ao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_topics = example_multiple_choices_questions['Topic']\n",
        "raw_questions = example_multiple_choices_questions['Question']\n",
        "raw_options = example_multiple_choices_questions['Multi-Choice Options']\n",
        "raw_correct_answers = example_multiple_choices_questions['Correct Answer']"
      ],
      "metadata": {
        "id": "amPUHXxfsDBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generating MCQAs with in-context example"
      ],
      "metadata": {
        "id": "5cOl4H5VsHTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_chat_completion_mcqa(user_prompt1, user_prompt2, assistant_prompt, system_prompt,\n",
        "                             engine='gpt-4-1106-preview',\n",
        "                             temperature=0, max_tokens=3000, top_p=0,\n",
        "                             frequency_penalty=0, presence_penalty=0, stop=None):\n",
        "    \"\"\"\n",
        "    Generates a chat completion using OpenAI's GPT model.\n",
        "\n",
        "    Parameters:\n",
        "    - user_prompt (str): The user's input prompt to the model.\n",
        "    - system_prompt (str): The system's initial prompt setting the context for the model.\n",
        "    - engine (str): The model you are using: [gpt-4, gpt4-32k, gpt-35-turbo-0613]\n",
        "    - temperature (float): Controls randomness in the generation.\n",
        "    - max_tokens (int): The maximum number of tokens to generate in the completion.\n",
        "    - top_p (float): Nucleus sampling parameter controlling the size of the probability mass considered for token generation.\n",
        "    - frequency_penalty (float): How much to penalize new tokens based on their frequency.\n",
        "    - presence_penalty (float): How much to penalize new tokens based on their presence.\n",
        "    - stop (list or None): Tokens at which to stop generating further tokens.\n",
        "\n",
        "    Returns:\n",
        "    - str: The generated completion text.\n",
        "    \"\"\"\n",
        "    client = OpenAI(api_key=GPT4V_KEY)\n",
        "\n",
        "    message_text = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": user_prompt1},\n",
        "        {\"role\": \"assistant\", \"content\": assistant_prompt},\n",
        "        {\"role\": \"user\", \"content\": user_prompt2}\n",
        "    ]\n",
        "\n",
        "    completion = client.chat.completions.create(\n",
        "        model=engine,\n",
        "        messages=message_text,\n",
        "        temperature=temperature,\n",
        "        max_tokens=max_tokens,\n",
        "        top_p=top_p,\n",
        "        frequency_penalty=frequency_penalty,\n",
        "        presence_penalty=presence_penalty,\n",
        "        stop=stop,\n",
        "        # logprobs=True,\n",
        "        # top_logprobs=5\n",
        "    ).choices[0].message.content\n",
        "\n",
        "    return completion"
      ],
      "metadata": {
        "id": "IcILOOKfsDn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = 'You are a smart AI assistant. Given an example consent form and a multiple-choices question in a specific topic regarding the form that helps patient understand the form. Now generate new questions regarding the new form in the same topic.'\n",
        "pdf_list = pd.read_csv('/content/drive/MyDrive/ChatNCCN/cancer_icf_ids')\n",
        "for i, nct in enumerate(pdf_list):\n",
        "  pdf = os.path.join('/content/drive/MyDrive/ChatNCCN/cancer_icfs_21to24', f'{nct}.pdf')\n",
        "  nct_text = convert_pdf_to_text(pdf)\n",
        "  print(f'Processing {nct}........')\n",
        "  if nct_text == '':\n",
        "    print('Got empty text.........')\n",
        "    continue\n",
        "  nct_mcqa_dict_with_example = defaultdict(dict)\n",
        "  for t in intersect_set:\n",
        "    if t == 46:\n",
        "      continue\n",
        "    topic = raw_topics.iloc[t]\n",
        "    print(f'Processing {topic}........')\n",
        "    example_question_options = raw_questions.iloc[t] + '\\n ' + raw_options.iloc[t] + '\\n Correct answer: ' + raw_correct_answers.iloc[t]\n",
        "    user_prompt1 = f'===Example consent form===: \\n {example_consent_form} \\n\\n ===Topic===: \\n {topic} \\n Generate a multiple-choices question in the given topic regarding the given consent form.'\n",
        "    assistant_prompt = f'===Example question===: \\n {example_question_options}'\n",
        "    user_prompt2 = f'===New consent form===: \\n {nct_text} \\n\\n ===Topic===: \\n {topic} \\n Generate one multiple-choices question in the given topic regarding the new consent form; the correct option of each new question should not be the original sentences from the consent form.'\n",
        "    nct_mcqa_dict_with_example[topic] = get_chat_completion_mcqa(user_prompt1, user_prompt2, assistant_prompt, system_prompt).replace('===New question===:\\n', '')\n",
        "  topics = nct_mcqa_dict_with_example.keys()\n",
        "  questions = nct_mcqa_dict_with_example.values()\n",
        "  df = pd.DataFrame({'Topics': list(topics), 'Questions': questions})\n",
        "  df.to_csv(os.path.join(out_dir, f'{nct}_mcqa_gpt4_turbo_with_example.csv'))"
      ],
      "metadata": {
        "id": "5jN5Jt0OsN3K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}